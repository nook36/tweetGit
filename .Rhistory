install.packages("bitops")
install.packages("RCurl")
install.packages("RJSONIO")
EnsurePackage<-function(x)
{
x<-as.character(x)
if(!require(x,ccharacter.only = TRUE))
{
install.packages(pkgs=x,repos = "http://cran.r-project.org")
require(x,character.only = TRUE)
}
}
PrepareTwitter<-function()
{
EnsurePackage("bitops")
EnsurePackage("Rcurl")
EnsurePackage("RJSONIO")
EnsurePackage("twitterR")
}
PrepareTwitter<-function()
{
EnsurePackage("bitops")
EnsurePackage("Rcurl")
EnsurePackage("RJSONIO")
EnsurePackage("twitterR")
}
PrepareTwitter()
EnsurePackage<-function(x)
{
x<-as.character(x)
if(!require(x,character.only = TRUE))
{
install.packages(pkgs=x,repos = "http://cran.r-project.org")
require(x,character.only = TRUE)
}
}
PrepareTwitter()
PrepareTwitter<-function()
{
EnsurePackage("bitops")
EnsurePackage("Rcurl")
EnsurePackage("RJSONIO")
EnsurePackage("twitteR")
}
PrepareTwitter<-function()
{
EnsurePackage("bitops")
EnsurePackage("Rcurl")
EnsurePackage("RJSONIO")
EnsurePackage("twitteR")
}
PrepareTwitter()
tweetList<-searchTwitter("#climate",n=500)
searchTwitter()
searchTwitter("#climate",n-500)
searchTwitter("#climate",n=500)
tweetList<-searchTwitter("#climate",n=500)
EnsurePackage<-function(x)
{
x<-as.character(x)
if(!require(x,character.only = TRUE))
{
install.packages(pkgs=x,repos = "http://cran.r-project.org")
require(x,character.only = TRUE)
}
}
PrepareTwitter<-function()
{
EnsurePackage("bitops")
EnsurePackage("Rcurl")
EnsurePackage("RJSONIO")
EnsurePackage("twitteR")
}
tweetList<-searchTwitter("#climate",n=500)
PrepareTwitter()
searchTwitter("#climate",n=500)
api_key<-"YnzQqVlEDfdhsSyrijrRTNA6B"
api_secret<-"7lAFSDcjvmgTOG7oten8VisvcNfZzXSGzyymgUjiEejV6smsvv"
access_token<-"2907568728-cqM1a5ZY5WkX8doaf27BWHi6vcyhDGBiNjvIW1x"
access_token_secret<-"Zf5cHN3Qa3BLRtQOl1Ex0yzxWnIZvLLCzufSIGAljQkiQ"
setup_twitter_oauth(api_key,api_secret,access_token,access_token_secret)
searchTwitter("#climate",n=500)
PrepareTwitter()
PrepareTwitter()
PrepareTwitter<-function()
{
EnsurePackage("bitops")
EnsurePackage("RCurl")
EnsurePackage("RJSONIO")
EnsurePackage("twitteR")
}
PrepareTwitter()
tweetlist<-searchTwitter("#cimate",n=500)
setup_twitter_oauth(api_key,api_secret,access_token,access_token_secret)
tweetlist<-searchTwitter("#climate",n=500)
tweetlist
PrepareTwitter()
tweetList<-searchTwitter("#climate",n=500)
setup_twitter_oauth(api_key,api_secret,access_token,access_token_secret)
tweetList<-searchTwitter("#climate",n=500)
tweetList
mode(tweetList)
str(tweetList)
str(head(tweetList,1))
tweetDF<-do.call("rbind",lapply(tweetList,as.data.frame))
tweetDF
#TweetFrame()-Return a dataframe based on a search of Twitter
TweetFrame<-function(searchTerm,maxTweets)
{
twtList<-searchTwitter(searchTerm,n=maxTweets)
return(do.call("rbind",lapply(twtList,as.data.frame)))
}
mdData<-TweetFrame("#mindfulness",250)
mdData
tweetDF
tweetDF$created
attach(tweetDF)
head(created,2)
hist(created,breaks=15,freq=TRUE)
replicate(400,mean(sample(USstatePops$V1,size=16,replace = TRUE)simplify=TRUE))
USstatePops$V1
USstatePops$V1
USStatePops
load("~/2.0 Eng_2012/Reference material/Data_Science/rStudio-DatascienceBook1.R")
USstatePops<-read.DIF("clipboard",transpose = TRUE)
USstatePops
USstatePops
data.set<-c(7,7,8,8,7,8,9())
data.set<-c(7,7,8,8,7,9)
data()
samps<-combn(data.set,2)
samps
data.set
help combn()
combn(c(1,2,3),2)
data.set<-c(7,7,8,8,7,8,9)
samps<-combn(data.set,2)
samps
combn(c(1,2,3),3)
xbars<-colMeans(samps)
xbars
table(xbars)
prop.table(table(xbars))
barplot(table(xbars))
help combn
table(c(1,1,3,4,5,5,6,8))
prop.table(table(c(1,1,3,4,5,5,6,8)))
barplot(table(c(1,1,1,2,3,4,4,5,5,5,5,5)))
MySamplingDistribution<function(myvector,nTime,nSize)
{
hist(replicate(ntime,mean(sample(myvector,nsize,replace=TRUE)),simplify = TRUE))
}
MySamplingDistribution<-function(myvector,nTime,nSize)
{
hist(replicate(ntime,mean(sample(myvector,nsize,replace=TRUE)),simplify = TRUE))
}
MySamplingDistribution(USstatePops$V1,1000,16)
MySamplingDistribution<-mysamfunction(myvector,nTime,nSize)
{
hist(replicate(nTime,mean(sample(myvector,nsize,replace=TRUE)),simplify = TRUE))
}
MySamplingDistribution<-mysamfunction(myvector,nTime,nSize)
{
hist(replicate(nTime,mean(sample(myvector,nsize,replace=TRUE)),simplify = TRUE))
}
MySamplingDistribution(USstatePops$V1,1000,16)
MySamplingDistribution(USstatePops$V1,1000,16)
MySamplingDistribution<-mysamfunction(myvector,nTime,nSize)
{
hist(replicate(nTime,mean(sample(myvector,nsize,replace=TRUE)),simplify = TRUE))
}
MySamplingDistribution<-function(myvector,nTime,nSize)
{
hist(replicate(nTime,mean(sample(myvector,nsize,replace=TRUE)),simplify = TRUE))
}
MySamplingDistribution(USstatePops$V1,1000,16)
MySamplingDistribution<-function(myvector,nTime,nSize)
{
hist(replicate(nTime,mean(sample(myvector,nSize,replace=TRUE)),simplify = TRUE))
}
MySamplingDistribution(USstatePops$V1,1000,16)
MySamplingDistribution<-function(myvector,nTime,nSize)
{
hist(replicate(nTime,mean(sample(myvector,size=nSize,replace=TRUE)),simplify = TRUE))
}
MySamplingDistribution(USstatePops$V1,1000,16)
MySamplingDistribution<-function(myvector,nTime,nSize)
{
#hist(replicate(nTime,mean(sample(myvector,size=nSize,replace=TRUE)),simplify = TRUE))
return(sample(myvector,size=nSize,replace=TRUE))
}
MySamplingDistribution(USstatePops$V1,1,4)
{
#hist(replicate(nTime,mean(sample(myvector,size=nSize,replace=TRUE)),simplify = TRUE))
return(mean(sample(myvector,size=nSize,replace=TRUE)))
}
MySamplingDistribution<-function(myvector,nTime,nSize)
{
#hist(replicate(nTime,mean(sample(myvector,size=nSize,replace=TRUE)),simplify = TRUE))
return(mean(sample(myvector,size=nSize,replace=TRUE)))
}
MySamplingDistribution(USstatePops$V1,100,16)
MySamplingDistribution(USstatePops$V1,1000,4)
MySamplingDistribution<-function(myvector,nTime,nSize)
{
#hist(replicate(nTime,mean(sample(myvector,size=nSize,replace=TRUE)),simplify = TRUE))
return(mean(sample(myvector,size=nSize,replace=TRUE)))
}
USstatePops$V1
MySamplingDistribution(USstatePops$V1,1000,8)
MySamplingDistribution<-function(myvector,nSize)
{
#hist(replicate(nTime,mean(sample(myvector,size=nSize,replace=TRUE)),simplify = TRUE))
return(mean(sample(myvector,size=nSize,replace=TRUE)))
}
MySamplingDistribution(USstatePops$V1,8)
MySamplingDistribution<-function(myvector,nSize)
{
#hist(replicate(nTime,mean(sample(myvector,size=nSize,replace=TRUE)),simplify = TRUE))
#return(mean(sample(myvector,size=nSize,replace=TRUE)))
return(sample(myvector,size=nSize,replace = TRUE))
}
MySamplingDistribution(USstatePops$V1,4)
samp<-mysamplingdistribution(USstatePops$V1,4)
samp<-MysamplingDistribution(USstatePops$V1,4)
samp<-MySamplngDistribution(USstatePops$V1,4)
samp<-MySamplingDistribution(USstatePops$V1,4)
mean(samp)
d<-c(1,1,2,2)
mean(d)
samp
sample(c(1,2,1,2))
sample(c(1,3,6,1,3))
sample(c(1,3,1,4,6),size=3,replace=TRUE)
sample(USstatePops$V1,size=8,replace=TRUE)
meam(sample(USstatePops$V1,size=8,replace=TRUE))
mean(sample(USstatePops$V1,size=8,replace=TRUE))
help mean()
help(mean)
mean(sample(USstatePops$V1,szie=16,replace = TRUE))
mean(sample(USstatePops$V1,size=16,replace = TRUE))
help("mean")
mean(sample(USstatePops$V1,size=16,replace=TRUE),na.rm=TRUE)
help(lapply)
help(sapply)
lapply(sample(USstatePops$V1,size=16,replace=TRUE),mean,na.rm=TRUE)
warnings()
USstatePops$V1
USstatePops<-read.DIF("clipboard",transpose = TRUE)
mean(sample(USstatePops$V1,size=16,replace = TRUE))
MySamplingDistribution<-function(myvector,nTime,nSize)
{
hist(replicate(nTime,mean(sample(myvector,size=nSize,replace=TRUE)),simplify = TRUE))
#return(mean(sample(myvector,size=nSize,replace=TRUE)))
#return(sample(myvector,size=nSize,replace = TRUE))
}
MySamplingDistribution(USstatePops$V1,1000,16)
setup_twitter_oauth(api_key,api_secret,access_token,access_token_secret)
PrepareTwitter()
setup_twitter_oauth(api_key,api_secret,access_token,access_token_secret)
rpois(10,3)
mean(rpois(100,3))
var(rpois(100,3))
hist(rpois(1000,3))
PrepareTwitter()
ssetup_twitter_oauth(access_token,access_token_secret,api_key,api_secret)
setup_twitter_oauth(access_token,access_token_secret,api_key,api_secret)
setup_twitter_oauth(api_key,api_secret,access_token,access_token_secret)
tweetList<-searchtwitter("#halliburton",n=500)
tweetList<-searchTwitter("#halliburton",n=500)
tweetList
tweetList
str(tweetlist())
str(tweetList)
str(head(tweetList))
tweetDF<-do.call("rbind",lapply(tweetList,as.data.frame))
setup_twitter_oauth(api_key,api_secret,access_token,access_token_secret)
PrepareTwitter()
setup_twitter_oauth(api_key,api_secret,access_token,access_token_secret)
tweetDF
tweetDF$created
attach(tweetDF)
head(created,4)
hist(created,breaks=15,freq=TRUE)
sortweetDF<-tweetDF[order(as.integer(created)),]
detach(tweetDF)
attach(sortweetDF)
diff(created)
hist(as.integer(diff(created)))
library("modeest")
help.start
mfv(as.integer(diff(created)))
median(as.integer(diff(created)))
plot(DelayProbability(rpois(100,10),1,20),col=2)
#Like ArrivaProbability, but works with unsorted list of delay times
DelayProbability<-function(delays,increment,max)
{
#Initialize empty vector
plist<-NULL
#Probability is defined over the size of this sample of arrival times
delayLen<-length(delays)
#May not be necessary, nut checks for input mistake
if (increment>MAX) {return(NULL)}
for (i in seq(increment,max,by=increment))
(
#logical test <=i provides list of TRUEs and FALSEs of lenght = timeLen, then sum() counts the TRUEs
plist<-c(plist,(sum(delays<=1)/delayLen))
)
return(plist)
}
plot(DelayProbability(100,10),1,20),col=2)
plot(DelayProbability(rposi(100,10),1,20)col=2)
plot(DelayProbability(rpois(100,10)1,20),col=2)
plot(DelayProbability(rpois(100,10),1,20),col=2)
#Like ArrivaProbability, but works with unsorted list of delay times
DelayProbability<-function(delays,increment,max)
{
#Initialize empty vector
plist<-NULL
#Probability is defined over the size of this sample of arrival times
delayLen<-length(delays)
#May not be necessary, nut checks for input mistake
if (increment>max) {return(NULL)}
for (i in seq(increment,max,by=increment))
(
#logical test <=i provides list of TRUEs and FALSEs of lenght = timeLen, then sum() counts the TRUEs
plist<-c(plist,(sum(delays<=1)/delayLen))
)
return(plist)
}
plot(DelayProbability(rpois(100,10),1,20),col=2)
points(DelayProbability(rpois(100,3),1,20),col=3)
DelayProbability(rpois(100,10)1,20)
DelayProbability(rpois(100,10),1,20)
rpois(100,10)
DelayProbability(rpois(100,10),1,20)
rpois(100,10)
length(rpois(100,10))
#Like ArrivaProbability, but works with unsorted list of delay times
DelayProbability<-function(delays,increment,max)
{
#Initialize empty vector
plist<-NULL
#Probability is defined over the size of this sample of arrival times
delayLen<-length(delays)
#May not be necessary, nut checks for input mistake
if (increment>max) {return(NULL)}
for (i in seq(increment,max,by=increment))
(
#logical test <=i provides list of TRUEs and FALSEs of lenght = timeLen, then sum() counts the TRUEs
plist<-c(plist,(sum(delays<=i)/delayLen))
)
return(plist)
}
plot(DelayProbability(rpois(100,10),1,20),col=2)
plot(DelayProbability(rpois(100,3),1,20),col=3)
plot(DelayProbability(rpois(100,10),1,20),col=2)
points(DelayProbability(rpois(100,3),1,20),col=3)
for (1 in 1:15){points(DelayProbability(rpois(100,10),1,20))}
for (i in 1:15){points(DelayProbability(rpois(100,10),1,20))}
plot(1,20,xlim=c(0,20),ylim=c(0,1))
plot("xis","yxis",xlim=c(0,20),ylim=c(0,1))
plot(xis,yis,xlim=c(0,20),ylim=c(0,1))
for(i,1:20){points(i,ppois(i,lambda=10))}
for (i in 1:20){points(i,ppois(i,lambda=10))}
sum(rpois(100000,10)<=9)/100000
sum(rpois(100000,10)<=19)/100000
qpois(ppois(10,lambda = 10),lambda=10)
poisson.test(58638,100000)
EnsurePackage("gplots")
barplot2(c(0.66,0.146),ci.l=c(0.596,0.114),ci.u=c(0.742,0.184),plot.ci = TRUE,names.arg = c("Gaga","Oprah"))
poisson.test(c(333,73),c(500,500))
EnsurePackage("stringr")
tweetList
tweetDF
View(PrepareTwitter)
tweetDF <- TweetFrame("#solar",100)
PrepareTwitter()
View(TweetFrame)
searchTwitter("#solar",100)
setup_twitter_oauth(api_key,api_secret,access_token,access_token_secret)
api_key
api_secret
PrepareTwitter()
setup_twitter_oauth(api_key,api_secret,access_token,access_token_secret)
tweetDF<-TweetFrame("#solar",100)
head(TweetFrame(),1)
head(tweetDF,1)
head(tweetDF$text,2)
View(tweetDF)
search()
detach(sortweetDF)
attach(tweetDF)
search()
str_length(text)
EnsurePackage("stringR")
EnsurePackage("stringr")
search
search()
EnsurePackage("KernSmooth")
search()
str_length(text)
tail(text,1)
str_enc_toutf8(text)
stri_enc_toutf8(text)
tweetDF$textlen<-str-length(text)
tweetDF$textlen<-str_length(text)
detach(tweetDF)
attach(tweetDF)
tweetDF(textlen>140,"text")
tweetDF[textlen>140,"text"]
tweetDF[textlen=140,"text"]
tweetDF[textlen>139,"text"]
Sys.which("git")
tweetDF$modtext<-str_replace_all(text,"  "," ")
search()
EnsurePackage("stringr")
search()
attach(tweetDF)
search()
tweetDF$modtext<-str_replace_all(text,"  "," ")
tweetDF$textlen2<-str_length(tweetDF$modtext)
tweetDF[textlen != textlen2]
tweetDF[textlen != textlen2,]
detach(tweetDF)
attach(tweetDF)
tweetDF[textlen != textlen2,]
Sys.which(git)
Sys.which("git")
save.image("~/2.0 Eng_2012/Reference material/Data_Science/R-Pjs/twitter/tspace.RData")
